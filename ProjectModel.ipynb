{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4KLNcdbiZoWO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data from the CSV file\n",
        "df = pd.read_excel('dataset/combinedextended.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y3LyFPxoHae"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[['Frequency', 'Amplitude']]\n",
        "y = df['Mass']\n",
        "\n",
        "# \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuEwuF6XrhqW"
      },
      "source": [
        "Linear Interpolation for Amplitude for different Mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yu4nOyZmw-Wy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Mass</th>\n",
              "      <th>Amplitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>0.4999</td>\n",
              "      <td>0.001485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0.9998</td>\n",
              "      <td>0.001570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>1.4997</td>\n",
              "      <td>0.001656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>1.9996</td>\n",
              "      <td>0.001741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2902</th>\n",
              "      <td>30</td>\n",
              "      <td>158.9682</td>\n",
              "      <td>0.500652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903</th>\n",
              "      <td>30</td>\n",
              "      <td>159.4681</td>\n",
              "      <td>0.502855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2904</th>\n",
              "      <td>30</td>\n",
              "      <td>159.9680</td>\n",
              "      <td>0.505059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>30</td>\n",
              "      <td>160.4679</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2906</th>\n",
              "      <td>30</td>\n",
              "      <td>160.9678</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2907 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Frequency      Mass  Amplitude\n",
              "0             8    0.0000   0.001400\n",
              "1             8    0.4999   0.001485\n",
              "2             8    0.9998   0.001570\n",
              "3             8    1.4997   0.001656\n",
              "4             8    1.9996   0.001741\n",
              "...         ...       ...        ...\n",
              "2902         30  158.9682   0.500652\n",
              "2903         30  159.4681   0.502855\n",
              "2904         30  159.9680   0.505059\n",
              "2905         30  160.4679   0.505200\n",
              "2906         30  160.9678   0.505200\n",
              "\n",
              "[2907 rows x 3 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# get unique frequency values\n",
        "freqs = df['Frequency'].unique()\n",
        "\n",
        "# create an empty list to store the interpolated dataframes\n",
        "interp_dfs = []\n",
        "\n",
        "# loop through each frequency and interpolate missing mass values\n",
        "for freq in freqs:\n",
        "    # get data for this frequency\n",
        "    freq_df = df[df['Frequency'] == freq]\n",
        "\n",
        "    # sort the data by mass\n",
        "    freq_df = freq_df.sort_values('Mass')\n",
        "\n",
        "    # get mass and amplitude values\n",
        "    masses = freq_df['Mass'].values\n",
        "    amplitudes = freq_df['Amplitude'].values\n",
        "\n",
        "    # create a new mass array with equal spacing\n",
        "    new_masses = np.arange(masses[0], masses[-1]+1, 0.4999)\n",
        "\n",
        "    # interpolate amplitude values for the new mass array\n",
        "    new_amplitudes = np.interp(new_masses, masses, amplitudes)\n",
        "\n",
        "    # create a new dataframe with interpolated values\n",
        "    interp_df = pd.DataFrame({'Frequency': freq, 'Mass': new_masses, 'Amplitude': new_amplitudes})\n",
        "\n",
        "    # append the interpolated data to the list\n",
        "    interp_dfs.append(interp_df)\n",
        "\n",
        "# concatenate the interpolated dataframes into a single dataframe\n",
        "interp_df = pd.concat(interp_dfs, ignore_index=True)\n",
        "\n",
        "# merge the interpolated data with the original data\n",
        "df = pd.concat([df, interp_df], ignore_index=True)\n",
        "\n",
        "# remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "interp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear Interpolation for Amplitude and Unbalance Force for different Mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Mass</th>\n",
              "      <th>Amplitude</th>\n",
              "      <th>Unbalance Force</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>1.000000e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>0.4999</td>\n",
              "      <td>0.001485</td>\n",
              "      <td>1.051578e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0.9998</td>\n",
              "      <td>0.001570</td>\n",
              "      <td>2.103057e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>1.4997</td>\n",
              "      <td>0.001656</td>\n",
              "      <td>3.154535e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>1.9996</td>\n",
              "      <td>0.001741</td>\n",
              "      <td>4.206014e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2902</th>\n",
              "      <td>30</td>\n",
              "      <td>158.9682</td>\n",
              "      <td>0.500652</td>\n",
              "      <td>4.689881e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903</th>\n",
              "      <td>30</td>\n",
              "      <td>159.4681</td>\n",
              "      <td>0.502855</td>\n",
              "      <td>4.710582e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2904</th>\n",
              "      <td>30</td>\n",
              "      <td>159.9680</td>\n",
              "      <td>0.505059</td>\n",
              "      <td>4.731283e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>30</td>\n",
              "      <td>160.4679</td>\n",
              "      <td>0.505200</td>\n",
              "      <td>4.732608e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2906</th>\n",
              "      <td>30</td>\n",
              "      <td>160.9678</td>\n",
              "      <td>0.505200</td>\n",
              "      <td>4.732608e-02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2907 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Frequency      Mass  Amplitude  Unbalance Force\n",
              "0             8    0.0000   0.001400     1.000000e-09\n",
              "1             8    0.4999   0.001485     1.051578e-05\n",
              "2             8    0.9998   0.001570     2.103057e-05\n",
              "3             8    1.4997   0.001656     3.154535e-05\n",
              "4             8    1.9996   0.001741     4.206014e-05\n",
              "...         ...       ...        ...              ...\n",
              "2902         30  158.9682   0.500652     4.689881e-02\n",
              "2903         30  159.4681   0.502855     4.710582e-02\n",
              "2904         30  159.9680   0.505059     4.731283e-02\n",
              "2905         30  160.4679   0.505200     4.732608e-02\n",
              "2906         30  160.9678   0.505200     4.732608e-02\n",
              "\n",
              "[2907 rows x 4 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# get unique frequency values\n",
        "freqs = df['Frequency'].unique()\n",
        "\n",
        "# create an empty list to store the interpolated dataframes\n",
        "interp_dfs = []\n",
        "\n",
        "# loop through each frequency and interpolate missing mass values\n",
        "for freq in freqs:\n",
        "    # get data for this frequency\n",
        "    freq_df = df[df['Frequency'] == freq]\n",
        "\n",
        "    # sort the data by mass\n",
        "    freq_df = freq_df.sort_values('Mass')\n",
        "\n",
        "    # get mass, amplitude, and unbalance force values\n",
        "    masses = freq_df['Mass'].values\n",
        "    amplitudes = freq_df['Amplitude'].values\n",
        "    unbalance_forces = freq_df['Unbalance Force'].values\n",
        "\n",
        "    # create a new mass array with equal spacing\n",
        "    new_masses = np.arange(masses[0], masses[-1]+1, 0.4999)\n",
        "\n",
        "    # interpolate amplitude and unbalance force values for the new mass array\n",
        "    new_amplitudes = np.interp(new_masses, masses, amplitudes)\n",
        "    new_unbalance_forces = np.interp(new_masses, masses, unbalance_forces)\n",
        "\n",
        "    # create a new dataframe with interpolated values\n",
        "    interp_df = pd.DataFrame({'Frequency': freq, 'Mass': new_masses, 'Amplitude': new_amplitudes, 'Unbalance Force': new_unbalance_forces})\n",
        "\n",
        "    # append the interpolated data to the list\n",
        "    interp_dfs.append(interp_df)\n",
        "\n",
        "# concatenate the interpolated dataframes into a single dataframe\n",
        "interp_df = pd.concat(interp_dfs, ignore_index=True)\n",
        "\n",
        "# merge the interpolated data with the original data\n",
        "df = pd.concat([df, interp_df], ignore_index=True)\n",
        "\n",
        "# remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "interp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the interpolated DataFrame to an Excel file\n",
        "interp_df.to_excel('inter-step.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "IcaTmdqD06wU",
        "outputId": "cdc9151c-1cf1-4cb5-f8b4-6f2a557c46c1"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "interp_df.plot(kind='scatter', x='Mass', y='Amplitude', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ryD1tWBzZy7",
        "outputId": "492966d2-7c9d-46ca-f83b-febc069276f0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# frequency to check accuracy\n",
        "freq = 30\n",
        "\n",
        "# get data for the given frequency\n",
        "freq_df = df[df['Frequency'] == freq]\n",
        "\n",
        "# sort the data by mass\n",
        "freq_df = freq_df.sort_values('Mass')\n",
        "\n",
        "# get mass and amplitude values\n",
        "masses = freq_df['Mass'].values\n",
        "amplitudes = freq_df['Amplitude'].values\n",
        "\n",
        "# create a new mass array with equal spacing\n",
        "new_masses = np.arange(masses[0], masses[-1]+1, 5)\n",
        "\n",
        "# interpolate amplitude values for the new mass array\n",
        "new_amplitudes = np.interp(new_masses, masses, amplitudes)\n",
        "\n",
        "# get the actual amplitude values for the interpolated mass values\n",
        "actual_amplitudes = freq_df[freq_df['Mass'].isin(new_masses)]['Amplitude'].values\n",
        "\n",
        "# calculate the RMSE between the interpolated and actual values\n",
        "rmse = np.sqrt(np.mean((new_amplitudes - actual_amplitudes)**2))\n",
        "\n",
        "print(f\"RMSE for frequency {freq}: {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_HLfQKoG2e"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkGd2r11aC40",
        "outputId": "4b072b17-362c-43c5-c02f-6f453feab709"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X = df[['Frequency', 'Amplitude']]\n",
        "y = df['Mass']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vS1mpJfdgeB"
      },
      "source": [
        "Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIcy43LLde7Q",
        "outputId": "804ab062-10fc-46aa-8104-47aa279f7f0a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# find the best degree of polynomial features using cross-validation\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "best_score = float('inf')\n",
        "best_degree = None\n",
        "for degree in degrees:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X_train)\n",
        "    model = LinearRegression()\n",
        "    scores = cross_val_score(model, X_poly, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    avg_score = np.mean(scores)\n",
        "    if avg_score < best_score:\n",
        "        best_score = avg_score\n",
        "        best_degree = degree\n",
        "\n",
        "print(\"Best degree: \", best_degree)\n",
        "\n",
        "# create polynomial features with the best degree\n",
        "poly = PolynomialFeatures(degree=best_degree)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# fit the model on the training set\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "# make predictions on the test set\n",
        "y_pred = model.predict(X_test_poly)\n",
        "\n",
        "# evaluate the model on the test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdN00KO8o4x9",
        "outputId": "96ea0243-34e5-4b8e-a749-e727714bed88"
      },
      "outputs": [],
      "source": [
        "# make a prediction on a sample value\n",
        "sample_X = np.array([[12, 0.007]])\n",
        "sample_X_poly = poly.transform(sample_X)\n",
        "sample_y_pred = model.predict(sample_X_poly)\n",
        "print(\"Predicted mass: \", sample_y_pred[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DscWMGOsdlFc"
      },
      "source": [
        "Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UlaJgyKdpNQ",
        "outputId": "c29e71de-2918-4a65-cfce-c404b3b36495"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# create a range of alpha values to try\n",
        "alphas = np.logspace(-4, 4, 100)\n",
        "\n",
        "# use cross-validation to find the best alpha value\n",
        "ridge = RidgeCV(alphas=alphas, cv=5)\n",
        "ridge.fit(X, y)\n",
        "\n",
        "# make predictions\n",
        "y_pred = ridge.predict(X)\n",
        "\n",
        "# evaluate the model\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmJCeEO6dwwa"
      },
      "source": [
        "Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s5g6BY_dz4M",
        "outputId": "87747c03-75d0-438f-df4a-04e1d9f1f184"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# create a range of alpha values to try\n",
        "alphas = np.logspace(-4, 4, 100)\n",
        "\n",
        "# use cross-validation to find the best alpha value\n",
        "lasso = LassoCV(alphas=alphas, cv=5)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# make predictions\n",
        "y_pred = lasso.predict(X)\n",
        "\n",
        "# evaluate the model\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tHZAO_jd1Gm"
      },
      "source": [
        "Elastic Net Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_7cfQr0d8PL"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# create a range of alpha values to try\n",
        "alphas = np.logspace(-4, 4, 100)\n",
        "\n",
        "# create a range of l1_ratio values to try\n",
        "l1_ratios = np.linspace(0, 1, 100)\n",
        "\n",
        "# use cross-validation to find the best alpha and l1_ratio values\n",
        "elastic_net = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5)\n",
        "elastic_net.fit(X, y)\n",
        "\n",
        "# make predictions\n",
        "y_pred = elastic_net.predict(X)\n",
        "\n",
        "# evaluate the model\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXqGa5rd_At"
      },
      "source": [
        "Random Forest Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omuP6dwCd9WW"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# create a range of n_estimators values to try\n",
        "n_estimators = [10, 50, 100, 200, 500]\n",
        "\n",
        "X = df[['Frequency', 'Amplitude']]\n",
        "y = df['Mass']\n",
        "\n",
        "# find the best n_estimators value using cross-validation\n",
        "best_rmse = float('inf')\n",
        "best_n = 0\n",
        "for n in n_estimators:\n",
        "    rf = RandomForestRegressor(n_estimators=n, random_state=42)\n",
        "    scores = cross_val_score(rf, X, y, cv=10, scoring='neg_mean_squared_error')\n",
        "    rmse_scores = np.sqrt(-scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    if avg_rmse < best_rmse:\n",
        "        best_rmse = avg_rmse\n",
        "        best_n = n\n",
        "\n",
        "print(\"Best number of estimators: \", best_n)\n",
        "print(\"Best RMSE: \", best_rmse)\n",
        "\n",
        "# train the model on the full dataset using the best number of estimators\n",
        "rf = RandomForestRegressor(n_estimators=best_n, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# test the model on a new sample value\n",
        "sample_X = np.array([[17, 0.0508]])  # replace with your sample value\n",
        "sample_y_pred = rf.predict(sample_X)\n",
        "print(\"Predicted mass: \", sample_y_pred[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC9p5FxVoM1K"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjnQOQ3yoKbE",
        "outputId": "93281858-b170-4e82-d924-dcd295c5b99e"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "# create a Decision Tree regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7iIncGbqTzX"
      },
      "outputs": [],
      "source": [
        "# prompt: test on entire data and add result coloumn and print\n",
        "\n",
        "# Test the model on the entire dataset\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Add a new column to the DataFrame with the predicted values\n",
        "df['Predicted Mass'] = y_pred\n",
        "\n",
        "# Print the DataFrame\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu0SDTQCo-RC",
        "outputId": "afd89dc8-1562-4913-c064-b088b2db6047"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCNs_nwOoO8J"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6ROOt-XoQ-Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# create a Random Forest regressor with 100 trees\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mso6cn0oSi9"
      },
      "source": [
        "Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WerTZWF6oVoS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# create a Gradient Boosting regressor with 100 trees\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKRDCxA3oWjr"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EaoJcy6oYAw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# create an SVM regressor with a radial basis function kernel\n",
        "model = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='auto')\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEW7_zxoZWW"
      },
      "source": [
        "Neural Networks: feedforward neural network, also known as a multilayer perceptron (MLP).\n",
        "\n",
        "To predict Mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-uzp5lqobM5",
        "outputId": "76c50824-7ac6-4ab9-b4cd-1480a4e127c9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = interp_df[['Frequency', 'Amplitude']]\n",
        "y = interp_df['Mass']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(2,))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=6552, validation_data=(X_test_scaled, y_test_scaled), verbose=0)\n",
        "\n",
        "loss = model.evaluate(X_test_scaled, y_test_scaled)\n",
        "print('Test loss:', loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural Networks: feedforward neural network, also known as a multilayer perceptron (MLP).\n",
        "\n",
        "To predict Mass and unbalance force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 1.1121e-04\n",
            "Test loss: 0.00010528010170673952\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming interp_df now has a new column 'UnbalanceForce'\n",
        "X = interp_df[['Frequency', 'Amplitude']]\n",
        "y = interp_df[['Mass', 'Unbalance Force']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(2,))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(2)  # Changed to 2 neurons for two output variables\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train_scaled, epochs=6552, validation_data=(X_test_scaled, y_test_scaled), verbose=0)\n",
        "\n",
        "loss = model.evaluate(X_test_scaled, y_test_scaled)\n",
        "print('Test loss:', loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Cannot clone object '<Sequential name=sequential, built=True>' (type <class 'keras.src.models.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# fit the grid search object to the training data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print the best random_state value and corresponding mean squared error\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest random_state value: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:882\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    879\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    880\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m--> 882\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[0;32m    886\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    887\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    888\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    895\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    896\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:91\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39m__sklearn_clone__()\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:113\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    108\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m             )\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[0;32m    118\u001b[0m             )\n\u001b[0;32m    120\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m    121\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mTypeError\u001b[0m: Cannot clone object '<Sequential name=sequential, built=True>' (type <class 'keras.src.models.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# define a range of random_state values to search over\n",
        "param_grid = {'random_state': [0, 10, 20, 30, 42]}\n",
        "\n",
        "# create a grid search object with the model and parameter grid\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# fit the grid search object to the training data\n",
        "grid_search.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# print the best random_state value and corresponding mean squared error\n",
        "print(\"Best random_state value: \", grid_search.best_params_['random_state'])\n",
        "print(\"Best mean squared error: \", -grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAWcz3P5-rp4",
        "outputId": "a5c81616-1dfb-47fc-d0e7-fd723fee95a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Predicted mass: 161.6523\n",
            "Predicted unbalance force: 0.017109312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "new_data = np.array([[18, 0.1818]])\n",
        "new_data_scaled = scaler_X.transform(new_data)\n",
        "prediction_scaled = model.predict(new_data_scaled)\n",
        "prediction = scaler_y.inverse_transform(prediction_scaled)\n",
        "print('Predicted mass:', prediction[0][0])\n",
        "print('Predicted unbalance force:', prediction[0][1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Save the entire model (architecture, optimizer, and learned weights)\n",
        "model.save('ffnn.keras')\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the scalers\n",
        "with open('scaler_X.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler_X, file)\n",
        "\n",
        "with open('scaler_y.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler_y, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.01057547883794937\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "rmse = math.sqrt(loss)\n",
        "print('Test RMSE:', rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step\n",
            "Test MAE: 0.6063883085959927\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Test MAE:', mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R-squared: 0.9985561914621985\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R-squared:', r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Auto encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X = df[['Frequency', 'Amplitude']]\n",
        "y = df['Mass']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Assuming X_train contains only healthy data (mass = 0)\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "\n",
        "# Autoencoder model\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 16\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(input_dim,))\n",
        "encoder = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
        "decoder = tf.keras.layers.Dense(input_dim, activation='linear')(encoder)\n",
        "autoencoder = tf.keras.Model(input_layer, decoder)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history = autoencoder.fit(X_train_scaled, X_train_scaled, epochs=1000, verbose=0)\n",
        "\n",
        "# Anomaly detection function\n",
        "def detect_anomaly(autoencoder, scaler_X, input_data, threshold):\n",
        "    input_data_scaled = scaler_X.transform(input_data)\n",
        "    reconstruction = autoencoder.predict(input_data_scaled)\n",
        "    reconstruction_error = np.mean(np.square(input_data_scaled - reconstruction), axis=1)\n",
        "\n",
        "    is_anomaly = reconstruction_error > threshold\n",
        "    return is_anomaly\n",
        "\n",
        "# Set an appropriate threshold based on the reconstruction error distribution on the healthy data\n",
        "threshold = 0.005  # This value should be determined based on the reconstruction error distribution on the healthy data\n",
        "\n",
        "# Example usage\n",
        "new_data = np.array([[8, 0.0021]])\n",
        "is_anomaly = detect_anomaly(autoencoder, scaler_X, new_data, threshold)\n",
        "print(\"Anomaly detected:\" if is_anomaly else \"System is healthy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate reconstruction errors for healthy training data\n",
        "reconstruction_errors = np.mean(np.square(X_train_scaled - autoencoder.predict(X_train_scaled)), axis=1)\n",
        "\n",
        "# Calculate the 95th percentile of the reconstruction error distribution\n",
        "threshold = np.percentile(reconstruction_errors, 95)\n",
        "print(\"Threshold:\", threshold)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
